* FlowCL
A graph-oriented, optionally-typed, optionally-verifiable language built over Common Lisp.

*WARNING: FlowCL is currently in beta! Both the code itself and the paradigm it aims to crystallize in code are subject to change.*
+ In addition, the project structure is in flux, and it is currently published primarly to get external overviews, criticism, and recommendations on relevant materials (e.g. similar projects or writing on similar concepts). This notification will be removed from the README when enough features have been implemented to provide an idea of how v1 of the API should be designed.

The Flow language is a long-term project on my part, attempting to extend Common Lisp (and hopefully other runtimes in the future) with the ability to interactively build programs around the flow of information.

There are a few core concepts which guide the design and implementation of Flow:
- Graph-oriented: Flow is intended to [TBC]
- ...

FlowCL is intended to integrate with its host languages, allowing integration into existing codebases.

** Design principles
Some key principles of the FlowCL project:

- *Compatibility*: FlowCL strives to be compatible with code, libraries, and paradigms used by the framework's users.

  Functionality will be encapsulated as much as possible (e.g. within function/procedure/object definitions), and external endpoints can ideally be added directly to your code with no change in operation or design (save, hopefully, for increased ease-of-coding and potentially also performance).

  FlowCL will also attempt to use libraries such as "trivial-extensible-sequences" to make drop-in replacement even easier, by enabling compatibility with code that was designed for non-FlowCL structures.

- *Scalability*: FlowCL is meant to be a basis upon which small and large projects are built. As such, it should allow composition of its elements into larger structures without significant compromises. Ease of use will be a particular target to maintain as code or data structures are composed into larger structures.

  Another target of this goal is reliability. In a mature state, FlowCL will match the reliability of production level code, and be usable in the same.

  Ideally, FlowCL code should also provide efficiency boosts to the user. In some cases this will be outweighed by other factors, such as ease of use. However, we aim to identify opportunities for our code to optimize not only based on its own structure, but also based on information gleaned from how it is being used in a larger context, allowing us to automate a significant portion of the optimization work that would normally be done manually by programmers. This is the same approach used by software such as TensorFlow, to allow users to easly build large systems without unnecessarily large hardware costs or loss of performance.

- *Ease of use*: FlowCL aims for developers to easily use it.

  In part, this is a longer-term goal to implement documentation and tutorials to ease comprehension and adoption of the project. In the shorter term, this aspect will be fulfilled primarly through clear, understandable naming and documentation within the code.

  This goal also applies to the design of the framework itself. Endpoints intended for external use should be designed to easily integrate with each other and external code, such as through the creation of utilities for creating or manipulating FlowCL constructs (note that compatibility with native constructs would be a very nice feature for such utilities). In addition, where possible internal functionality should be constructed in such a way as to allow users to easily modify the system, without constantly breaking things or having to deal with excessive or obtuse interdependencies.

- *Extensibility*: One of the strengths of CL is its strong culture of backward-compatibility. However, sometimes this can also impede the ability to modify and improve a project.

  There are two main approaches to solve this issue. Firstly, we will design symbols intended as external endpoints to be as independent as possible from the internal implementations. This should allow us more freedom to improve the FlowCL project and the code of its users without any action on their part.

  Secondly, we will not limit ourselves to publishing a single package. Once the FlowCL project has matured enough to have a stable API, we will suffix package names with the "v0.01" convention. The latest stable sub-version will be aliased to its version, and the latest stable version will be aliased to un-suffixed "default" package names (i.e. the package "flow-cl-v0.01" will be aliased to "flow-cl-v0", which will be aliased to "flow-cl"). Many projects implement this process via Github branches, but our approach allows both active developers and users to track package versions without dealing with branches or commit history, making it significantly easier for package users to contribute improvements.
  + NOTE: "Major version" package names (e.g. "v1") are intended to be retained indefinitely. However, the minor versions underlying them (e.g. "v0.01", "v0.02", etc) are very likely to be discarded, and are mainly for development purposes. Once FlowCL is mature enough for the above system to be implemented, we *strongly recommend either pinning code to a major version or just using the default packages*.

** Installation
Place this repository somewhere ASDF or Quicklisp can find it as a local project, and then use `(ql:quickload :flow-cl)` (or `(asdf:load-system :flow-cl)`, which quicklisp wraps in its quickload feature).

** Usage

** Progress and Roadmap

Future plans are currently vague, but currently involve the following points:
- [-] Develop a prototype for node/graph computation
  - [-] Develop an extensible node abstraction
    - [X] Figure out the basic conceptual structure of the core node class
      - Currently thinking about making this just a directed graph of inputs/outputs with a ~value~ slot for node data/metadata
    - [ ] Implement a decent prototype of the core node class
    - [ ] Figure out extension points
    - [ ] Figure out how to define behavior for specific node subclasses
    - [ ] Figure out how to contain and execute computations in a node
    - [ ] Figure out how to traverse nodes by their connections
    - [ ] Figure out how to execute a network of nodes
  - [-] Develop some proofs-of-concept for the usefulness of the node abstraction in normal CL code
    - [-] Reactive programming
      - [X] Prototype exists
      - [ ] Update after the node semantics are tied down
    - [-] Syntax for making node programs in CL code
      - [X] Prototype exists
      - [ ] Update after the node semantics are tied down
    - [ ] Compiling program dataflow
      - Possible implementation: implement a simple from-scratch neural network and auto-parallelize and auto-optimize
  - [ ] Develop a compilation paradigm for networks of nodes
    - [ ] Figure out how to find patterns in graphs and replace them with more efficient alternatives
    - [ ] Figure out how to denote compilation behavior of individual nodes in relation to their contexts in a graph
      - [ ] Figure out what kinds of behavior and relevant aspects might be needed
      - [ ] Figure out a way to implement compilation and aspect-tracking in an extensible manner
      - [ ] Figure out how to actually denote the compilation behavior
      - [ ] Figure out how to deal with different behavior of different nodes/node-classes
    - [ ] Figure out how to choose between multiple possible compilation directions
  - [ ] Develop an extensible object/class abstraction for graphs
    - A graph is intended to be a collection of nodes with aggregate behavior.
    - [ ] Figure out how to collect nodes in a graph and refer to a graph's components
      - [ ] Figure out how to store the nodes
      - [ ] Figure out how to access a graph's components with reasonable efficiency
      - [ ] Figure out how to execute a graph
      - [ ] Figure out how to compile a graph
        - [ ] Figure out how to denote compilation characteristics of graphs
        - [ ] Figure out how graph compilation interacts with e.g. one graph being contained by another
    - [ ] Figure out graph-specific operations
      - [ ] Comparison between graphs
        - Should probably have a way to determine if graphs have the same structure, and if so call them equal
      - [ ] Copying graphs
      - [ ] Including one graph in another
        - [ ] How do we do this without sharing the same graph object across instances?
        - [ ] How do we update the included graphs if the original changes?
        - [ ] Compilation
  - [ ] Develop a way for graph programs to modify themselves, i.e. graph macros
    - [ ] Figure out the semantics of execution times and graph compilation
    - [ ] Figure out how to formalize the concept of modifiers on existing nodes/graphs/graph-segments
    - [ ] Figure out how to formalize the concept of
- [ ] Develop a framework for execution contexts
  - [ ] Tracking the current execution environment as a graph is traversed
    - [ ] Storing the execution environment
    - [ ] Representing computations that read/write to the execution environment
    - [ ] Efficiency
    - [ ] Figuring out the semantics to denote "universes" in the graph
      - [ ] Syntax for this?
      - [ ] How will this actually work?
        - Look at Lean and similar languages for inspiration here?
    - [ ] Inheritance between environments
      - [ ] Dealing with inherited values and their updates
      - [ ] Dealing with access syntax/semantics
      - [ ] Multiple inheritance
        - Most likely this will end up creating a DAG of environments
      - [ ] Representing the outside world (i.e. the host language and external environment) in the universe context
        - Likely will be the parent of everything else, with the Flow runtime creating a child universe for itself from which other universes can inherit
    - [ ] Moving execution between different places on the graph
      - How do I maintain the execution-context continuations in this case?
      - How do we get unwind-protect like functionality, for cases where
  - [ ] Parallel execution of graphs
    - This would need to be worked out along with the execution contexts, to avoid the latter obstructing the former and requiring hacky workarounds.
- [ ] Develop a dependent typing framework for graphs
  - [ ] Study dependent typing
  - [ ] Implement the framework for atoms, functions, macros, etc
  - [ ] Implement the framework for types
  - [ ] Implement the framework for universes
- [ ] Develop the user interface for this language
  - Needs to be done in tandem with the other steps
  - [ ] Syntax?
  - [ ] Representation of data/programs?
  - [ ] Restrictions on user behavior and their bypasses?

- Complete the lazy programming sub-framework?
  - This may not be worth it, compared to just starting anew
  - This may be better served in a separate library rather than as a part of the FlowCL implementation.

  - Improving the efficiency.

    Time-wise, the lazy-vector structure matches native code by applying functions to its internal vector cache, providing reasonable time efficiency.

    However, the space-efficiency of the laziness system is rather bad; for extremely large inputs, I've sometimes run into heap failures without explicit garbage-collection.
    We need to find ways to reduce the space taken by lazy structures
    + lazy-cons (which is also currently the backbone of lazy-vector) is a significant offender here; reimplementing it to use an internal list cache like lazy-vector, and perhaps using the cdr to store tails when lazy-consing multiple lazy sequences, might work to resolve this issue, and could also allow reusing the lazy-vector code which permits sharing of evaluation where possible.

      However, this has the issue that it forces the user to only use structures which cache their results. This is not necessarily a good thing, considering that they may be using lazy sequences to process data too large to actively keep in memory. Further thought is required here.

    + lazy-vector should benefit from explicitly using a function as its generator rather than shelling out to lazy-cons objects. lazy-vec may not even need to be reworked; if we can figure out another function template for lazy-vectors *not* derived from lazy-conses, we will likely not need to change the lazy-vector so much that lazy-vec can't just be tweaked a little to store and retrieve the input lazy-cons in the right place.

    + NOTE: If lazy-vector stops being a subclass of lazy-cons, we will need to rework the class architecture. I think a good approach would be to make a "lazy-sequence" object defining the shared slots and specializing most of the generic methods, with only the lowest-level methods directly specializing on different lazy sequences

    We also might want to optimize their garbage collection (perhaps using weak-pointers for reuse of defunct objects that haven't been garbage collected?).

  - Improving the feature-set

    Currently, the only features implemented are the core thunk class, and lazy-cons and lazy-vector as replacement sequences for lists and vectors.

    One direction for progress is incorporating streams into the library, allowing them to be wrapped as lazy-sequences. We may need to restrict this functionality considering that you can't go backwards in a stream, but our lazy stream object should at least allow the composition of mapping, filtration, etc that lazy sequences currently permit.

    + Perhaps this could be done via documentation suggesting restrictions on usage to avoid making calls that are supposed to refer to the same items in the stream, and/or creating a macro for stream interaction and which inherently contains such restrictions.

      The macro approach seems particularly interesting. If the user can specify a set of sequences which are meant to receive the data from the stream, then the macro could internally create a cache object which only retains those aspects of the stream which are necessary given the current state of the various sequences. Including any non-lazy sequences would force full reading and caching of the stream, but lazy-sequences would wait on cache reads and stream-reads (to extend the cache) based on which elements were forced to evaluate.

      Once no objects were waiting on a value of the macro's internal cache, that value could be discarded (possibly using the :offset value with periodic copy-seq commands, if using a lazy-vector?). This would allow a rolling track on only those values which are necessary, without the user needing to manually program such caching.

      However, this method requires either lazy sequences which discard data automatically (e.g. the current implementation of lazy-cons, if we make the old heads space-efficient enough to be feasible for production); or, preferably, a method to update the list of receiving sequences, so that the user can decide when to discard old data. Perhaps just allowing setf on the defined sequences would be enough for this.
      + Incidentally, having this feature in the core macro would make it much easier to implement a convenience macro on top of it, which tracks the size of the receiving sequences and automatically replaces them with copies (if the user is confident that only the current datapoint and/or a preset amount of history will be relevant).

    We also need to explicitly develop the design to optionally permit interaction without preservation of values (either directly in the code or through making it easy to discard cached information). This is necessary for processing of extremely large values (as mentioned in the musing on stream wrappers above), but seems at odds with the goal of maintaining performance (unless calculating the step function is *extremely* computation-heavy); further thought is required here.

  - Improving the integration.

    The lazy-programming framework is currently placed *literally just in the flow-cl package*. The first priority here is moving it to its own package and figuring out how the package architecture is going to work.

    After that, I need to figure out how users will interact with this system. Obviously one mode is to use the enclosed lazy sequences as drop-in sequence objects compatible with Common Lisp sequence functions (assuming your implementation is compatible with trivial-extensible-sequences, or you use the appropriate fallback code).

    Another mode would be to make specific use-cases for lazy-sequences (such as wrapping streams) and writing drop-in macros to implement that specific feature.

    Ideally, another mode will be added using a macro to implement a DSL for lazy programming. The result of code written in that DSL would be the output of the form, allowing for integration into normal CL code. However, *this is a minor goal*, and takes a backseat to working on other subsystems of the overall FlowCL framework.

- ...

** Author

+ Swapneil Singh

** Copyright

Copyright (c) 2023 Swapneil Singh

** License

Licensed under the MIT License.
